# ğŸ“‘ AI Document Intelligence

AI-powered document intelligence system with **RAG (Retrieval-Augmented Generation)**, **summarization**, and **question answering**.  
Supports PDF/TXT ingestion, vector search with FAISS, and multiple LLM backends (OpenAI / Hugging Face).

---

## ğŸš€ Features
- ğŸ“‚ Load and process **PDF** and **TXT** documents.
- âœ‚ï¸ Automatic **chunking** and embedding generation.
- ğŸ” Vector search with **FAISS**.
- ğŸ¤– **Question answering** over documents using RAG.
- ğŸ“ Document **summarization**.
- ğŸŒ Supports **OpenAI** and **Hugging Face** providers.
- ğŸ¨ Simple **Streamlit UI** for interaction.

---

## ğŸ“¦ Installation

### 1ï¸âƒ£ Clone repo
```bash
git clone https://github.com/yourusername/ai_doc_intel.git
cd ai_doc_intel

2ï¸âƒ£ Create virtual environment
python -m venv .venv
source .venv/bin/activate   # On Linux / macOS
.venv\Scripts\activate      # On Windows

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

âš™ï¸ Configuration
Create a .env file in the project root:
# Choose providers
EMBEDDING_PROVIDER=openai   # options: openai | hf
LLM_PROVIDER=openai         # options: openai | hf_local | hf_inference

# OpenAI
OPENAI_API_KEY=your-openai-key
OPENAI_EMBED_MODEL=text-embedding-3-large
OPENAI_CHAT_MODEL=gpt-4o-mini

# Hugging Face
HF_TOKEN=your-hf-token
HF_EMBED_MODEL=sentence-transformers/all-MiniLM-L6-v2
HF_GENERATION_MODEL=HuggingFaceH4/zephyr-7b-beta

# Chunking
CHUNK_SIZE=1200
CHUNK_OVERLAP=120

# Storage
VECTOR_DIR=storage/faiss_index
DOCS_DIR=storage/docs

# Summarization
SUMMARY_MAX_TOKENS=800


â–¶ï¸ Running the App
Start Streamlit UI
streamlit run frontend/app.py


ğŸ“‚ Project Structure
ai_doc_intel/
â”‚â”€â”€ backend/
â”‚   â”œâ”€â”€ document_loader.py   # Load PDFs / TXTs
â”‚   â”œâ”€â”€ vectorstore.py       # FAISS vector DB
â”‚   â”œâ”€â”€ rag_chain.py         # Retrieval + QA pipeline
â”‚   â”œâ”€â”€ summarizer.py        # Summarization logic
â”‚   â”œâ”€â”€ utils.py             # Embedding + LLM utils
â”‚   â””â”€â”€ config.py            # Loads .env configs
â”‚
â”‚â”€â”€ frontend/
â”‚   â””â”€â”€ app.py               # Streamlit interface
â”‚
â”‚â”€â”€ storage/
â”‚   â”œâ”€â”€ docs/                # Uploaded docs
â”‚   â””â”€â”€ faiss_index/         # Vector store
â”‚
â”‚â”€â”€ .env                     # API keys & settings
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md


ğŸ“Œ Usage

Upload a PDF/TXT file in the UI.
The system chunks and embeds the content.
You can:
    â“ Ask questions about the document.
    ğŸ“ Generate a summary.

ğŸ”® Roadmap
    Support for DOCX
    Multi-document RAG
    Advanced UI with chat history
    Docker deployment

ğŸ“œ License

MIT License Â© 2025 Antick Bhattacharjee